# Landmark Segmentation with Foundation Models

This repository contains a simplified and anonymized preview of a work-in-progress project under AAAI 2026 submission.

This is a **preview only**. The full method, and data are currently under academic peer review and will be released upon acceptance.


## Description
We explore the adaptation of vision foundation models (e.g. SAM2, Depth Anything V2) for depth-aware landmark segmentation in laparoscopic imagery.

### ✅ Features shown
- Mock RGB & depth fusion data pipeline
- Inference pipeline using pretrained SAM2 (no fine-tuned weights)
- Visualization of contour extraction


## ✅ What's Included
- Mock data for RGB and depth input
- Demo inference script using pretrained SAM2 API
- Sample output visualization


### ❌ Not included
- Custom fusion architecture details
- Fine-tuning scripts or training data
- Real surgical images

** For code access under NDA or interview context, please contact: `lyunchen178@gmail.com`
